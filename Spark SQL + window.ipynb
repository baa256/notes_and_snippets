{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718c4a3-e4b1-45e6-afd3-f9b107208f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a window function operates on a set of rows and return a value for each row in the set \n",
    "\n",
    "LEAD function lets youu query more than one rowq in a table at a time wihtout having to join the table to itself.\n",
    "\n",
    "\n",
    "SELECT train_station, time, \n",
    "LEAD(time,1) OVER (PARTITION BY train ORDER BY time) as time_next\n",
    "FROM schedule \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada7447-cc98-4bb6-b004-25631ebcc730",
   "metadata": {},
   "outputs": [],
   "source": [
    "Running sums using window function SQL\n",
    "\n",
    "# Add col running_total that sums diff_min col in each group\n",
    "query = \"\"\"\n",
    "SELECT train_id, station, time, diff_min,\n",
    "SUM(diff_min) OVER (PARTITION BY train_id ORDER BY time) AS running_total\n",
    "FROM schedule\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and display the result\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15cad8-24c7-4408-a4e9-ef9290ec6c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aggregation\n",
    "\n",
    "# Give the identical result in each command\n",
    "spark.sql('SELECT train_id, MIN(time) AS start FROM schedule GROUP BY train_id').show()\n",
    "df.groupBy('train_id').agg({'time':'min'}).withColumnRenamed('min(time)', 'start').show()\n",
    "\n",
    "# Print the second column of the result\n",
    "spark.sql('SELECT train_id, MIN(time), MAX(time) FROM schedule GROUP BY train_id').show()\n",
    "result = df.groupBy('train_id').agg({'time':'min', 'time':'max'})\n",
    "result.show()\n",
    "print(result.columns[1])\n",
    "\n",
    "\n",
    "# Write a SQL query giving a result identical to dot_df\n",
    "query = \"SELECT train_id, MIN(time) AS start, MAX(time) AS end FROM schedule GROUP BY train_id\"\n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()\n",
    "\n",
    "\n",
    "--\n",
    "\n",
    "# Obtain the identical result using dot notation \n",
    "dot_df = df.withColumn('time_next', lead('time', 1)\n",
    "        .over(Window.partitionBy('train_id')\n",
    "        .orderBy('time')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9a86c-dcbe-4d70-9a80-911b9fdf6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SQL query to obtain an identical result to dot_df \n",
    "query = \"\"\"\n",
    "SELECT *, \n",
    "(UNIX_TIMESTAMP(LEAD(time, 1) OVER (PARTITION BY train_id ORDER BY time),'H:m') \n",
    " - UNIX_TIMESTAMP(time, 'H:m'))/60 AS diff_min \n",
    "FROM schedule \n",
    "\"\"\"\n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12ba86-a6da-4020-b024-499b90a26fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Moving/ Sliding Window Functions \n",
    "\n",
    "# Load the dataframe\n",
    "df = spark.read.load('sherlock_sentences.parquet')\n",
    "\n",
    "# Filter and show the first 5 rows\n",
    "df.where('id > 70').show(5, truncate=False)\n",
    "\n",
    "# Split the clause column into a column called words \n",
    "split_df = clauses_df.select(split('clause', ' ').alias('words'))\n",
    "split_df.show(5, truncate=False)\n",
    "\n",
    "# Explode the words column into a column called word \n",
    "exploded_df = split_df.select(explode('words').alias('word'))\n",
    "exploded_df.show(10)\n",
    "\n",
    "# Count the resulting number of rows in exploded_df\n",
    "print(\"\\nNumber of rows: \", exploded_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b306ddc1-72df-47c5-ada1-b165c804a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving window \n",
    "\n",
    "# Word for each row, previous two and subsequent two words\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "part,\n",
    "LAG(word, 2) OVER(PARTITION BY part ORDER BY id) AS w1,\n",
    "LAG(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\n",
    "word AS w3,\n",
    "LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) AS w4,\n",
    "LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) AS w5\n",
    "FROM text\n",
    "\"\"\"\n",
    "spark.sql(query).where(\"part = 12\").show(10)\n",
    "\n",
    "# Repartition text_df into 12 partitions on 'chapter' column\n",
    "repart_df = text_df.repartition(12, 'chapter')\n",
    "\n",
    "# Prove that repart_df has 12 partitions\n",
    "repart_df.rdd.getNumPartitions()\n",
    "\n",
    "\n",
    "# Find the top 10 sequences of five words\n",
    "query = \"\"\"\n",
    "SELECT w1, w2, w3, w4, w5, COUNT(*) AS count FROM (\n",
    "   SELECT word AS w1,\n",
    "   LEAD(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\n",
    "   LEAD(word,2) OVER(PARTITION BY part ORDER BY id ) AS w3,\n",
    "   LEAD(word,3) OVER(PARTITION BY part ORDER BY id ) AS w4,\n",
    "   LEAD(word,4) OVER(PARTITION BY part ORDER BY id ) AS w5\n",
    "   FROM text\n",
    ")\n",
    "GROUP BY w1, w2, w3, w4, w5\n",
    "ORDER BY count DESC\n",
    "LIMIT 10\n",
    "\"\"\" \n",
    "df = spark.sql(query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce6194-8169-4446-ad4a-0f2e315ca7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique 5-tuples in sorted order\n",
    "\n",
    "# Unique 5-tuples sorted in descending order\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT w1, w2, w3, w4, w5 FROM (\n",
    "   SELECT word AS w1,\n",
    "   LEAD(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\n",
    "   LEAD(word,2) OVER(PARTITION BY part ORDER BY id ) AS w3,\n",
    "   LEAD(word,3) OVER(PARTITION BY part ORDER BY id ) AS w4,\n",
    "   LEAD(word,4) OVER(PARTITION BY part ORDER BY id ) AS w5\n",
    "   FROM text\n",
    ")\n",
    "ORDER BY w1 DESC, w2 DESC, w3 DESC, w4 DESC, w5 DESC \n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df = spark.sql(query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ba3e1-3ffd-4260-89f1-3360a5c663eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Most frequent 3-tuple per chapter\n",
    "query = \"\"\"\n",
    "SELECT chapter, w1, w2, w3, count FROM\n",
    "(\n",
    "  SELECT\n",
    "  chapter,\n",
    "  ROW_NUMBER() OVER (PARTITION BY chapter ORDER BY count DESC) AS row,\n",
    "  w1, w2, w3, count\n",
    "  FROM ( %s )\n",
    ")\n",
    "WHERE row = 1\n",
    "ORDER BY chapter ASC\n",
    "\"\"\" % subquery\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a853a-a900-43b1-837f-bbf85cb0d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caching \n",
    "\n",
    "# Unpersists df1 and df2 and initializes a timer\n",
    "prep(df1, df2) \n",
    "\n",
    "# Cache df1\n",
    "df1.cache()\n",
    "\n",
    "# Run actions on both dataframes\n",
    "run(df1, \"df1_1st\") \n",
    "run(df1, \"df1_2nd\")\n",
    "run(df2, \"df2_1st\")\n",
    "run(df2, \"df2_2nd\", elapsed=True)\n",
    "\n",
    "# Prove df1 is cached\n",
    "print(df1.is_cached)\n",
    "\n",
    "\n",
    "# Unpersist df1 and df2 and initializes a timer\n",
    "prep(df1, df2) \n",
    "\n",
    "# Persist df2 using memory and disk storage level \n",
    "df2.persist(storageLevel=pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Run actions both dataframes\n",
    "run(df1, \"df1_1st\") \n",
    "run(df1, \"df1_2nd\")\n",
    "run(df2, \"df2_1st\")\n",
    "run(df2, \"df2_2nd\", elapsed=True)\n",
    "\n",
    "\n",
    "# List the tables\n",
    "print(\"Tables:\\n\", spark.catalog.listTables())\n",
    "\n",
    "# Cache table1 and Confirm that it is cached\n",
    "spark.catalog.cacheTable('table1')\n",
    "print(\"table1 is cached: \", spark.catalog.isCached('table1'))\n",
    "\n",
    "# Uncache table1 and confirm that it is uncached\n",
    "spark.catalog.uncacheTable('table1')\n",
    "print(\"table1 is cached: \", spark.catalog.isCached('table1'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca5fa83-6147-4be8-b0f7-f077b09436aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Spark UI\n",
    "\n",
    "Task - a unit of execution that runs on a single cpu \n",
    "Stage - a group of tasks that perform the same computation in parallel, each task typically running on a different subset of the data\n",
    "Job - a computation triggered by an action, sliced into one or more stages  \n",
    "\n",
    "usually at port 4040 by default \n",
    "http://[DRIVER_HOST]:4040\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e8f62b-ff33-4652-8d1a-b496fb292926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

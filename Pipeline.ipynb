{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e55747-9357-4910-9a69-caec5c98eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a csv file and set the headers\n",
    "df = (spark.read\n",
    "      .options(header=True)\n",
    "      .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6d7eb-16ac-44f5-904c-be870d60bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "schema = StructType([\n",
    "  StructField(\"brand\", StringType(), nullable=False),\n",
    "  StructField(\"model\", StringType(), nullable=False),\n",
    "  StructField(\"absorption_rate\", ByteType(), nullable=True),\n",
    "  StructField(\"comfort\", ByteType(), nullable=True)\n",
    "])\n",
    "\n",
    "better_df = (spark\n",
    "             .read\n",
    "             .options(header=\"true\")\n",
    "             # Pass the predefined schema to the Reader\n",
    "             .schema(schema)\n",
    "             .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
    "pprint(better_df.dtypes)\n",
    "\n",
    "\n",
    "# Specify the option to drop invalid rows\n",
    "ratings = (spark\n",
    "           .read\n",
    "           .options(header=True, mode=\"DROPMALFORMED\")\n",
    "           .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings_with_invalid_rows.csv\"))\n",
    "ratings.show()\n",
    "\n",
    "# Filling unknown data\n",
    " \n",
    "\n",
    "print(\"BEFORE\")\n",
    "ratings.show()\n",
    "\n",
    "print(\"AFTER\")\n",
    "# Replace nulls with arbitrary value on column subset\n",
    "ratings = ratings.fillna(4, subset=[\"comfort\"])\n",
    "ratings.show()\n",
    "\n",
    "--\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Add/relabel the column\n",
    "categorized_ratings = ratings.withColumn(\n",
    "    \"comfort\",\n",
    "    # Express the condition in terms of column operations\n",
    "    when(col(\"comfort\") > 3, \"sufficient\").otherwise(\"insufficient\"))\n",
    "\n",
    "categorized_ratings.show()\n",
    "\n",
    "--\n",
    "\n",
    "# Selecting and renaming columns\n",
    " \n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Select the columns and rename the \"absorption_rate\" column\n",
    "result = ratings.select([col(\"brand\"),\n",
    "                         col(\"model\"),\n",
    "                         col(\"absorption_rate\").alias(\"absorbency\")])\n",
    "\n",
    "# Show only unique values\n",
    "result.distinct().show()\n",
    "\n",
    "--\n",
    "\n",
    "from pyspark.sql.functions import col, avg, stddev_samp, max as sfmax\n",
    "\n",
    "aggregated = (purchased\n",
    "              # Group rows by 'Country'\n",
    "              .groupBy(col('Country'))\n",
    "              .agg(\n",
    "                # Calculate the average salary per group\n",
    "                avg('Salary').alias('average_salary'),\n",
    "                # Calculate the standard deviation per group and rename\n",
    "                stddev_samp('Salary'),\n",
    "                # Retain the highest salary per group and rename\n",
    "                sfmax('Salary').alias('highest_salary')\n",
    "              )\n",
    "             )\n",
    "\n",
    "aggregated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d4d68-41d9-4516-80ea-0c0b9660966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a deployable artifact\n",
    "\n",
    "\n",
    "cd spark_pipelines\n",
    "zip --recurse-paths pydiaper.zip pydiaper\n",
    "\n",
    "spark-submit --py-files PY_FILES MAIN_PYTHON_FILE\n",
    "spark-submit --py-files spark_pipelines/pydiaper/pydiaper.zip ./spark_pipelines/pydiaper/pydiaper/cleaning/clean_ratings.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3e1bc-c024-4102-8fde-c48ea3efbf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating in-memory DataFrames\n",
    "\n",
    "\n",
    "from datetime import date\n",
    "from pyspark.sql import Row\n",
    "\n",
    "Record = Row(\"country\", \"utm_campaign\", \"airtime_in_minutes\", \"start_date\", \"end_date\")\n",
    "\n",
    "# Create a tuple of records\n",
    "data = (\n",
    "  Record(\"USA\", \"DiapersFirst\", 28, date(2017, 1, 20), date(2017, 1, 27)),\n",
    "  Record(\"Germany\", \"WindelKind\", 31, date(2017, 1, 25), None),\n",
    "  Record(\"India\", \"CloseToCloth\", 32, date(2017, 1, 25), date(2017, 2, 2))\n",
    ")\n",
    "\n",
    "# Create a DataFrame from these records\n",
    "frame = spark.createDataFrame(data)\n",
    "frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284dcbe2-c662-4840-b855-0422dc563935",
   "metadata": {},
   "outputs": [],
   "source": [
    " from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, sum\n",
    "\n",
    "from .catalog import catalog\n",
    "\n",
    "\n",
    "def extract_demographics(sparksession, catalog):\n",
    "    return sparksession.read.parquet(catalog[\"clean/demographics\"])\n",
    "\n",
    "\n",
    "def store_chinese_demographics(frame, catalog):\n",
    "    frame.write.parquet(catalog[\"business/chinese_demographics\"])\n",
    "\n",
    "\n",
    "# Improved aggregation function, grouped by country and province\n",
    "def aggregate_inhabitants_by_province(frame):\n",
    "    return (frame\n",
    "            .groupBy(\"country\", \"province\")\n",
    "            .agg(sum(col(\"inhabitants\")).alias(\"inhabitants\"))\n",
    "            )\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    frame = extract_demographics(spark, catalog)\n",
    "    chinese_demographics = frame.filter(lower(col(\"country\")) == \"china\")\n",
    "    aggregated_demographics = aggregate_inhabitants_by_province(chinese_demographics)\n",
    "    store_chinese_demographics(aggregated_demographics, catalog)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f03eca-93be-44d1-83bb-e27cf0ffb92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "version: 2\n",
    "jobs:\n",
    "  build:\n",
    "    working_directory: ~/data_scientists/optimal_diapers/\n",
    "    docker:\n",
    "      - image: gcr.io/my-companys-container-registry-on-google-cloud-123456/python:3.6.4\n",
    "    steps:\n",
    "      - checkout\n",
    "      - run:\n",
    "          command: |\n",
    "            sudo pip install pipenv\n",
    "            pipenv install\n",
    "      - run:\n",
    "          command: |\n",
    "            pipenv run flake8 .\n",
    "            pipenv run pytest .\n",
    "      - store_test_results:\n",
    "          path: test-results\n",
    "      - store_artifacts:\n",
    "          path: test-results\n",
    "          destination: tr1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fad7219-0493-4d19-a082-e57ca0e58152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da94de-4b8e-4f24-9d0d-571d7e4e3649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a470c-718f-4762-8132-a0ce7f22542a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c96211-08d9-4c67-8240-53d3dcf765ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59d31f-83b5-4119-a88e-1e9c7d17b102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf10014-e141-49a5-a1e9-730e57f13094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1346ba8-183f-4b75-a446-248ee21197f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e080fb3-fecc-415d-96e9-a599d2d01eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
